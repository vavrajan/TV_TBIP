{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":470043,"status":"ok","timestamp":1649226107620,"user":{"displayName":"paul hofmarcher","userId":"12977868574614852692"},"user_tz":-120},"id":"xyyrwCK4Wq65","outputId":"613084ad-a6fa-48f9-e366-f073787f1bd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":4917,"status":"ok","timestamp":1649226112534,"user":{"displayName":"paul hofmarcher","userId":"12977868574614852692"},"user_tz":-120},"id":"d4bxIyA_Wu3-","outputId":"9f45fec6-a7d0-44e1-e4ea-1316987967af"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]},{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["import functools\n","import time\n","import os\n","import pandas as pd\n","import csv\n","import sys\n","\n","%tensorflow_version 1.x \n","import numpy as np\n","import scipy.sparse as sparse\n","from sklearn.decomposition import non_negative_factorization\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","\n","tf.reset_default_graph() #to resue tf variable \n","\n","#from sklearn.decomposition import NMF, non_negative_factorization \n","tf.test.gpu_device_name()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D24q9y9gWu9x"},"outputs":[],"source":["#parameters\n","num_topics = 25\n","max_steps = 300000\n","print_steps = 25000\n"," \n","## parameters for ADAM optimizer\n","eps = 1\n","learningrate = 0.0001\n","batch_size = 512\n","\n","path = '/content/gdrive/My Drive/tbip_BSP/data/'\n","#path to tbip.py, to change as needed\n","py_file = '/content/gdrive/My Drive/tbip_BSP/code/'  \n","#vocabulary\n","vocab = pd.read_csv(os.path.join(path,'vocabulary.txt'), header = None)\n","vocabulary = vocab[0].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfwhzuIaWu6-"},"outputs":[],"source":["sys.path.append(os.path.abspath(py_file))\n","import tbip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GypJiqoJWyiM","executionInfo":{"status":"ok","timestamp":1649295684224,"user_tz":-120,"elapsed":69569611,"user":{"displayName":"paul hofmarcher","userId":"12977868574614852692"}},"outputId":"65fda701-67f6-47f2-fa22-6531a9e9a744"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /content/gdrive/My Drive/tbip_BSP/code/tbip.py:135: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_probability/python/distributions/poisson.py:193: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/gdrive/My Drive/tbip_BSP/code/tbip.py:385: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","Learning for session 111\n","Step:   0 ELBO: -6307892.000 (1.679 sec/step)\n","Step: 25000 ELBO: -5322009.000 (0.058 sec/step)\n","Step: 50000 ELBO: -4721635.000 (0.058 sec/step)\n","Step: 75000 ELBO: -3889032.000 (0.058 sec/step)\n","Step: 100000 ELBO: -3658967.250 (0.058 sec/step)\n","Step: 125000 ELBO: -3547457.500 (0.058 sec/step)\n","Step: 150000 ELBO: -3504370.500 (0.058 sec/step)\n","Step: 175000 ELBO: -3726771.000 (0.058 sec/step)\n","Step: 200000 ELBO: -3227716.250 (0.058 sec/step)\n","Step: 225000 ELBO: -3448802.000 (0.058 sec/step)\n","Step: 250000 ELBO: -3322487.500 (0.058 sec/step)\n","Step: 275000 ELBO: -3384215.000 (0.058 sec/step)\n","Step: 299999 ELBO: -3476856.750 (0.058 sec/step)\n","session 111 done!\n","Learning for session 112\n","Step:   0 ELBO: -5758535.000 (1.437 sec/step)\n","Step: 25000 ELBO: -5110472.000 (0.058 sec/step)\n","Step: 50000 ELBO: -4780616.500 (0.058 sec/step)\n","Step: 75000 ELBO: -3999672.500 (0.058 sec/step)\n","Step: 100000 ELBO: -3464770.750 (0.058 sec/step)\n","Step: 125000 ELBO: -3277725.750 (0.058 sec/step)\n","Step: 150000 ELBO: -3354801.500 (0.058 sec/step)\n","Step: 175000 ELBO: -3441779.500 (0.058 sec/step)\n","Step: 200000 ELBO: -3432534.750 (0.058 sec/step)\n","Step: 225000 ELBO: -3301391.500 (0.058 sec/step)\n","Step: 250000 ELBO: -3367051.750 (0.058 sec/step)\n","Step: 275000 ELBO: -3134619.500 (0.058 sec/step)\n","Step: 299999 ELBO: -8437841.000 (0.058 sec/step)\n","session 112 done!\n","Learning for session 113\n","Step:   0 ELBO: -5260283.500 (1.330 sec/step)\n","Step: 25000 ELBO: -4438921.500 (0.058 sec/step)\n","Step: 50000 ELBO: -3893277.750 (0.058 sec/step)\n","Step: 75000 ELBO: -3473285.500 (0.058 sec/step)\n","Step: 100000 ELBO: -3183826.750 (0.058 sec/step)\n","Step: 125000 ELBO: -3296011.250 (0.058 sec/step)\n","Step: 150000 ELBO: -11550396.000 (0.058 sec/step)\n","Step: 175000 ELBO: -3331317.500 (0.058 sec/step)\n","Step: 200000 ELBO: -3037084.000 (0.058 sec/step)\n","Step: 225000 ELBO: -2833565.750 (0.058 sec/step)\n","Step: 250000 ELBO: -3037367.500 (0.058 sec/step)\n","Step: 275000 ELBO: -3014958.000 (0.058 sec/step)\n","Step: 299999 ELBO: -23288016.000 (0.058 sec/step)\n","session 113 done!\n","Learning for session 114\n","Step:   0 ELBO: -4778272.000 (1.101 sec/step)\n","Step: 25000 ELBO: -4024644.000 (0.058 sec/step)\n","Step: 50000 ELBO: -3358573.000 (0.058 sec/step)\n","Step: 75000 ELBO: -3170231.500 (0.058 sec/step)\n","Step: 100000 ELBO: -2903607.750 (0.058 sec/step)\n","Step: 125000 ELBO: -2801314.500 (0.058 sec/step)\n","Step: 150000 ELBO: -2548036.500 (0.058 sec/step)\n","Step: 175000 ELBO: -2683412.000 (0.058 sec/step)\n","Step: 200000 ELBO: -2686255.500 (0.058 sec/step)\n","Step: 225000 ELBO: -2726655.500 (0.058 sec/step)\n","Step: 250000 ELBO: -2579900.500 (0.058 sec/step)\n","Step: 275000 ELBO: -2660699.000 (0.058 sec/step)\n","Step: 299999 ELBO: -2697631.000 (0.058 sec/step)\n","session 114 done!\n"]}],"source":["#to start from session 98 onwards\n","for i in range(111, 115):\n","  tf.reset_default_graph() #to be used if graph building doesn't renew \n","  tf.set_random_seed(0) #after tf_reset_graph\n","  random_state = np.random.RandomState(0) #after tf_reset_graph\n","  old_dir = os.path.join(path, str(i-1), 'output' + str(max_steps))\n","  data_dir = os.path.join(path, str(i), 'input')\n","  save_dir = os.path.join(path, str(i), 'output' + str(max_steps))  # ~/data/<sess>/output\n","\n","  if not os.path.isdir(save_dir):\n","    os.mkdir(save_dir)\n","\n","  objective_topic_loc = np.load(os.path.join(old_dir, \"objective_topic_loc.npy\"))\n","  beta = objective_topic_loc\n","  beta = np.exp(beta)\n","\n","  X = sparse.load_npz(os.path.join(data_dir, 'counts.npz')) \n","  #X = X.astype('float64')\n","  W, H, n_iter = non_negative_factorization(X, n_components = num_topics, \n","                                            init='custom', \n","                                            random_state=0, \n","                                            update_H = False, \n","                                            H = beta.astype('float32')) #paul:we had astype('double') but returned error\n","  \n","  W = W.astype('float32')\n","  W = W + 1e-3 #causes 0 value otherwise\n","  initial_document_loc = W #theta\n","  objective_topic_loc_prev = beta.astype('float32') #don't need, use objective_topic_loc\n","\n","  ideological_topic_loc_prev = np.load(os.path.join(old_dir, 'ideological_topic_loc.npy')) #eta\n","  ideological_topic_loc_prev = tf.convert_to_tensor(ideological_topic_loc_prev)\n","\n","  #using tbip.build_input_pipeline\n","  #original source code available at: https://github.com/keyonvafa/tbip\n","  (iterator, author_weights, vocabulary, author_map, \n","  num_documents, num_words, num_authors) = tbip.build_input_pipeline(\n","        data_dir, \n","        batch_size,\n","        random_state,\n","        counts_transformation='nothing')\n","  document_indices, counts, author_indices = iterator.get_next()\n","\n","  # Create Lognormal variational family for document intensities (theta).\n","  document_loc = tf.get_variable(\n","      \"document_loc\",\n","      initializer=tf.constant(np.log(initial_document_loc))) #theta from NMF\n","\n","  document_scale_logit = tf.get_variable(\n","      \"document_scale_logit\",\n","      shape=[num_documents, num_topics],\n","      initializer=tf.initializers.random_normal(mean=-2, stddev=1.),\n","      dtype=tf.float32)\n","\n","  document_scale = tf.nn.softplus(document_scale_logit)\n","\n","  document_distribution = tfp.distributions.LogNormal(\n","      loc=document_loc,\n","      scale=document_scale) \n","\n","  # Create Lognormal variational family for objective topics (beta).\n","\n","  objective_topic_loc = tf.get_variable(\n","    \"objective_topic_loc\",\n","    initializer=tf.constant(np.log(objective_topic_loc_prev)), #from session 97\n","    trainable = True)\n","  \n","  objective_topic_scale_logit = tf.get_variable(\n","      \"objective_topic_scale_logit\",\n","      shape=[num_topics, num_words],\n","      initializer=tf.initializers.random_normal(mean=-2, stddev=1.),\n","      dtype=tf.float32,\n","      trainable = True)\n","  \n","  objective_topic_scale = tf.nn.softplus(objective_topic_scale_logit)\n","\n","  objective_topic_distribution = tfp.distributions.LogNormal(\n","      loc=objective_topic_loc,\n","      scale=objective_topic_scale)\n","    \n","  # Create Gaussian variational family for ideological topics (eta).\n","  \n","  ideological_topic_loc = tf.get_variable(\n","    \"ideological_topic_loc\",\n","    initializer = ideological_topic_loc_prev, #initialized only ideological_topic_loc so that var type doesn't change\n","    trainable = True)\n","  \n","  ideological_topic_scale_logit = tf.get_variable(\n","      \"ideological_topic_scale_logit\",\n","      shape=[num_topics, num_words],\n","      dtype=tf.float32,\n","      trainable = True)\n","  \n","  ideological_topic_scale = tf.nn.softplus(ideological_topic_scale_logit)\n","\n","  ideological_topic_distribution = tfp.distributions.Normal(\n","      loc=ideological_topic_loc,\n","      scale=ideological_topic_scale)\n","\n","  # Create Gaussian variational family for ideal points (x).\n","  ideal_point_loc = tf.get_variable(\n","      \"ideal_point_loc\",\n","      shape=[num_authors],\n","      dtype=tf.float32)\n","\n","  ideal_point_scale_logit = tf.get_variable(\n","      \"ideal_point_scale_logit\",\n","      initializer=tf.initializers.random_normal(mean=0, stddev=1.),\n","      shape=[num_authors],\n","      dtype=tf.float32)\n","\n","  ideal_point_scale = tf.nn.softplus(ideal_point_scale_logit)\n","\n","  ideal_point_distribution = tfp.distributions.Normal(\n","      loc=ideal_point_loc,\n","      scale=ideal_point_scale)\n","\n","  # Approximate ELBO.\n","  elbo = tbip.get_elbo(counts,\n","                      document_indices,\n","                      author_indices,\n","                      author_weights,\n","                      document_distribution,\n","                      objective_topic_distribution, \n","                      ideological_topic_distribution,\n","                      ideal_point_distribution,\n","                      num_documents,\n","                      batch_size)\n","  loss = -elbo\n","\n","\n","  optim = tf.train.AdamOptimizer(learning_rate=learningrate, epsilon=eps)\n","\n","  #original_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=1e-04)\n","  #optimizer = tf.contrib.estimator.clip_gradients_by_norm(original_optimizer, clip_norm=5.0)\n","  train_op = optim.minimize(loss)\n","\n","  document_mean = document_loc + document_scale ** 2 / 2 \n","\n","  neutral_mean = objective_topic_loc + objective_topic_scale ** 2 / 2\n","\n","  positive_mean = (objective_topic_loc + \n","                  ideological_topic_loc + \n","                  (objective_topic_scale ** 2 + \n","                    ideological_topic_scale ** 2) / 2) #intialized\n","                    \n","\n","  negative_mean = (objective_topic_loc - \n","                  ideological_topic_loc +\n","                  (objective_topic_scale ** 2 + \n","                    ideological_topic_scale ** 2) / 2) #intialized\n","  \n","  print('Learning for session ' + str(i))\n","  \n","  loss_vals = []\n","  init = tf.global_variables_initializer()\n","  sess = tf.Session()\n","  sess.run(init)\n","  start_time = time.time()\n","  for step in range(max_steps):\n","    (_, elbo_val) = sess.run([train_op, elbo])\n","    loss_vals.append(elbo_val)\n","    duration = (time.time() - start_time) / (step + 1)\n","    if step % print_steps == 0 or step == max_steps - 1:\n","      print(\"Step: {:>3d} ELBO: {:.3f} ({:.3f} sec/step)\".format(\n","          step, elbo_val, duration))\n","    if (step + 1) % 50000 == 0 or step == max_steps - 1:\n","      (document_topic_mean, neutral_topic_mean, negative_topic_mean, positive_topic_mean, \n","     ideal_point_mean, ots, otl, itl, its) = sess.run([document_mean, neutral_mean, negative_mean, \n","                                   positive_mean, ideal_point_loc, objective_topic_scale, \n","                                   objective_topic_loc, \n","                                   ideological_topic_loc, \n","                                   ideological_topic_scale])\n","      np.save(os.path.join(save_dir, \"document_topic_mean.npy\"), document_topic_mean)\n","      np.save(os.path.join(save_dir, \"neutral_topic_mean.npy\"), neutral_topic_mean)\n","      np.save(os.path.join(save_dir, \"negative_topic_mean.npy\"), negative_topic_mean)\n","      np.save(os.path.join(save_dir, \"positive_topic_mean.npy\"), positive_topic_mean)\n","      np.save(os.path.join(save_dir, \"ideal_point_mean.npy\"), ideal_point_mean)\n","      np.save(os.path.join(save_dir, \"objective_topic_scale.npy\"), ots)\n","      np.save(os.path.join(save_dir, \"objective_topic_loc.npy\"), otl)\n","      np.save(os.path.join(save_dir, \"ideological_topic_loc.npy\"), itl)\n","      np.save(os.path.join(save_dir, \"ideological_topic_scale.npy\"), its)        \n","      loss_df = pd.DataFrame(data={\"loss\": loss_vals}) \n","      loss_df.to_csv(os.path.join(save_dir, 'loss_values.csv'), sep=',', index=False) #save loss values as a csv file\n","\n","  print(\"session \" + str(i) + \" done!\")\n","  #i = i + 1"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"03_variational_initialization_beta_eta.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}