{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vrosn7vl29JD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648044527321,"user_tz":-60,"elapsed":1675,"user":{"displayName":"paul hofmarcher","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhhgUuaYX_GcoBd6nqqDFQOPR1zaZIWuY6wyJU=s64","userId":"12977868574614852692"}},"outputId":"f0163dcc-4bf1-4714-be34-b0dcc696eb80"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"HKUvUzZi3Ey2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648044529814,"user_tz":-60,"elapsed":2495,"user":{"displayName":"paul hofmarcher","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhhgUuaYX_GcoBd6nqqDFQOPR1zaZIWuY6wyJU=s64","userId":"12977868574614852692"}},"outputId":"8c0e61ab-95e9-4801-c9d2-3d3a37c0e629"},"source":["import functools\n","import time\n","import os\n","import csv\n","import sys\n","import pandas as pd\n","\n","%tensorflow_version 1.x #colab magic line to select Tensorflow 1.x\n","import numpy as np\n","import scipy.sparse as sparse\n","from sklearn.decomposition import NMF\n","import tensorflow as tf\n","import tensorflow_probability as tfp"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.x #colab magic line to select Tensorflow 1.x`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"id":"5IuAC9LO3E17","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1648044530160,"user_tz":-60,"elapsed":349,"user":{"displayName":"paul hofmarcher","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhhgUuaYX_GcoBd6nqqDFQOPR1zaZIWuY6wyJU=s64","userId":"12977868574614852692"}},"outputId":"0f0223fc-e781-4509-9f3b-f709bee6db9e"},"source":["tf.test.gpu_device_name() #shows /device:GPU:0"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"btSa1NNt3E-m"},"source":["num_topics = 25\n","tf.set_random_seed(0)\n","random_state = np.random.RandomState(0)\n","pre_initialize_parameters = True\n","#parameters for ADAM\n","eps = 1\n","learningrate = 0.0001\n","batch_size = 512\n","max_steps = 300000\n","print_steps = 25000\n","\n","#paths to ~data/97/input\n","data_dir = '/content/gdrive/My Drive/tbip_BSP/data/97/input'\n","#path to ~data/97/output\n","save_dir = os.path.join('/content/gdrive/My Drive/tbip_BSP/data/97/output' + str(max_steps)) #output folder indicates the number of iterations used\n","#path to tbip.py, to change as needed\n","py_file = '/content/gdrive/My Drive/tbip_BSP/code/'\n","\n","if not os.path.isdir(save_dir):\n","  os.mkdir(save_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYHyk8533E4w"},"source":["sys.path.append(os.path.abspath(py_file))\n","import tbip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VI8rmyjJ3FEc"},"source":["counts = sparse.load_npz(os.path.join(data_dir, 'counts.npz'))\n","num_documents,num_words = counts.shape\n","\n","if pre_initialize_parameters:\n","    nmf_model = NMF(n_components=num_topics,\n","                  init='random',\n","                  random_state=0,\n","                  max_iter=500)\n","    # Add offset to make sure none are zero.\n","    initial_document_loc = np.float32(nmf_model.fit_transform(counts) + 1e-3)\n","    initial_objective_topic_loc = np.float32(nmf_model.components_ + 1e-3)\n","\n","else:\n","    initial_document_loc = np.float32(\n","       np.exp(random_state.randn(num_documents, num_topics)))\n","    initial_objective_topic_loc = np.float32(\n","       np.exp(random_state.randn(num_topics, num_words)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJ3Cts-k3FHn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648044545221,"user_tz":-60,"elapsed":1140,"user":{"displayName":"paul hofmarcher","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhhgUuaYX_GcoBd6nqqDFQOPR1zaZIWuY6wyJU=s64","userId":"12977868574614852692"}},"outputId":"3d30c176-2f3d-42d5-bac4-99b2dac828ae"},"source":["(iterator, author_weights, vocabulary, author_map,\n"," num_documents, num_words, num_authors) = tbip.build_input_pipeline(\n","      data_dir,\n","      batch_size,\n","      random_state,\n","      counts_transformation='nothing')\n","document_indices, counts, author_indices = iterator.get_next()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /content/gdrive/My Drive/tbip_BSP/code/tbip.py:135: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"]}]},{"cell_type":"code","metadata":{"id":"oAVbC0Np3FKf"},"source":["# Create Lognormal variational family for document intensities (theta).\n","document_loc = tf.get_variable(\n","    \"document_loc\",\n","    initializer=tf.constant(np.log(initial_document_loc)))\n","document_scale_logit = tf.get_variable(\n","    \"document_scale_logit\",\n","    shape=[num_documents, num_topics],\n","    initializer=tf.initializers.random_normal(mean=-2, stddev=1.),\n","    dtype=tf.float32)\n","document_scale = tf.nn.softplus(document_scale_logit)\n","document_distribution = tfp.distributions.LogNormal(\n","    loc=document_loc,\n","    scale=document_scale)\n","\n","# Create Lognormal variational family for objective topics (beta).\n","objective_topic_loc = tf.get_variable(\n","    \"objective_topic_loc\",\n","    initializer=tf.constant(np.log(initial_objective_topic_loc)))\n","objective_topic_scale_logit = tf.get_variable(\n","    \"objective_topic_scale_logit\",\n","    shape=[num_topics, num_words],\n","    initializer=tf.initializers.random_normal(mean=-2, stddev=1.),\n","    dtype=tf.float32)\n","objective_topic_scale = tf.nn.softplus(objective_topic_scale_logit)\n","objective_topic_distribution = tfp.distributions.LogNormal(\n","    loc=objective_topic_loc,\n","    scale=objective_topic_scale)\n","\n","# Create Gaussian variational family for ideological topics (eta).\n","ideological_topic_loc = tf.get_variable(\n","    \"ideological_topic_loc\",\n","    shape=[num_topics, num_words],\n","    dtype=tf.float32)\n","ideological_topic_scale_logit = tf.get_variable(\n","    \"ideological_topic_scale_logit\",\n","    shape=[num_topics, num_words],\n","    dtype=tf.float32)\n","ideological_topic_scale = tf.nn.softplus(ideological_topic_scale_logit)\n","ideological_topic_distribution = tfp.distributions.Normal(\n","    loc=ideological_topic_loc,\n","    scale=ideological_topic_scale)\n","\n","# Create Gaussian variational family for ideal points (x).\n","ideal_point_loc = tf.get_variable(\n","    \"ideal_point_loc\",\n","    shape=[num_authors],\n","    dtype=tf.float32)\n","ideal_point_scale_logit = tf.get_variable(\n","    \"ideal_point_scale_logit\",\n","    initializer=tf.initializers.random_normal(mean=0, stddev=1.),\n","    shape=[num_authors],\n","    dtype=tf.float32)\n","ideal_point_scale = tf.nn.softplus(ideal_point_scale_logit)\n","ideal_point_distribution = tfp.distributions.Normal(\n","    loc=ideal_point_loc,\n","    scale=ideal_point_scale)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PURThjgx3FNp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648044545550,"user_tz":-60,"elapsed":331,"user":{"displayName":"paul hofmarcher","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhhgUuaYX_GcoBd6nqqDFQOPR1zaZIWuY6wyJU=s64","userId":"12977868574614852692"}},"outputId":"a707c45a-b945-433a-ccd8-60f5018c34b5"},"source":["# Approximate ELBO.\n","elbo = tbip.get_elbo(counts,\n","                     document_indices,\n","                     author_indices,\n","                     author_weights,\n","                     document_distribution,\n","                     objective_topic_distribution,\n","                     ideological_topic_distribution,\n","                     ideal_point_distribution,\n","                     num_documents,\n","                     batch_size)\n","loss = -elbo"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_probability/python/distributions/poisson.py:193: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/gdrive/My Drive/tbip_BSP/code/tbip.py:385: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"34UoIBU93FQu"},"source":["optim = tf.train.AdamOptimizer(learning_rate=learningrate, epsilon=eps)\n","train_op = optim.minimize(loss)\n","\n","document_mean = document_loc + document_scale ** 2 / 2\n","\n","neutral_mean = objective_topic_loc + objective_topic_scale ** 2 / 2\n","\n","positive_mean = (objective_topic_loc +\n","                 ideological_topic_loc +\n","                 (objective_topic_scale ** 2 +\n","                  ideological_topic_scale ** 2) / 2)\n","\n","negative_mean = (objective_topic_loc -\n","                 ideological_topic_loc +\n","                 (objective_topic_scale ** 2 +\n","                  ideological_topic_scale ** 2) / 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqOxzOjs3FTw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648081182285,"user_tz":-60,"elapsed":36636082,"user":{"displayName":"paul hofmarcher","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjhhgUuaYX_GcoBd6nqqDFQOPR1zaZIWuY6wyJU=s64","userId":"12977868574614852692"}},"outputId":"04d944a5-c634-4b6a-efeb-0fd0d2f2f2ce"},"source":["loss_vals = []\n","init = tf.global_variables_initializer()\n","sess = tf.Session()\n","sess.run(init)\n","start_time = time.time()\n","for step in range(max_steps):\n","  (_, elbo_val) = sess.run([train_op, elbo])\n","  duration = (time.time() - start_time) / (step + 1)\n","  loss_vals.append(elbo_val) #Keeping a track of elbo values\n","  if step % print_steps == 0 or step == max_steps - 1:\n","    print(\"Step: {:>3d} ELBO: {:.3f} ({:.3f} sec/step)\".format(\n","        step, elbo_val, duration))\n","  if (step + 1) % 50000 == 0 or step == max_steps - 1:\n","    (document_topic_mean, neutral_topic_mean, negative_topic_mean, positive_topic_mean,\n","     ideal_point_mean, ots, otl, itl, its) = sess.run([document_mean, neutral_mean, negative_mean,\n","                                   positive_mean, ideal_point_loc, objective_topic_scale, objective_topic_loc,\n","                                   ideological_topic_loc, ideological_topic_scale, ])\n","    np.save(os.path.join(save_dir, \"document_topic_mean.npy\"), document_topic_mean)\n","    np.save(os.path.join(save_dir, \"neutral_topic_mean.npy\"), neutral_topic_mean)\n","    np.save(os.path.join(save_dir, \"negative_topic_mean.npy\"), negative_topic_mean)\n","    np.save(os.path.join(save_dir, \"positive_topic_mean.npy\"), positive_topic_mean)\n","    np.save(os.path.join(save_dir, \"ideal_point_mean.npy\"), ideal_point_mean)\n","    np.save(os.path.join(save_dir, \"objective_topic_scale.npy\"), ots)\n","    np.save(os.path.join(save_dir, \"objective_topic_loc.npy\"), otl)\n","    np.save(os.path.join(save_dir, \"ideological_topic_loc.npy\"), itl)\n","    np.save(os.path.join(save_dir, \"ideological_topic_scale.npy\"), its)\n","    loss_df = pd.DataFrame(data={\"loss\": loss_vals})\n","    loss_df.to_csv(os.path.join(save_dir, 'loss_values.csv'), sep=',', index=False) #save loss values as a csv file\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step:   0 ELBO: -12745670.000 (3.057 sec/step)\n","Step: 25000 ELBO: -7278692.000 (0.122 sec/step)\n","Step: 50000 ELBO: -5975661.000 (0.122 sec/step)\n","Step: 75000 ELBO: -5733517.500 (0.122 sec/step)\n","Step: 100000 ELBO: -5202743.000 (0.122 sec/step)\n","Step: 125000 ELBO: -5153035.000 (0.122 sec/step)\n","Step: 150000 ELBO: -4787533.000 (0.122 sec/step)\n","Step: 175000 ELBO: -4827905.000 (0.122 sec/step)\n","Step: 200000 ELBO: -4772831.000 (0.122 sec/step)\n","Step: 225000 ELBO: -4744893.000 (0.122 sec/step)\n","Step: 250000 ELBO: -4737532.500 (0.122 sec/step)\n","Step: 275000 ELBO: -5018617.000 (0.122 sec/step)\n","Step: 299999 ELBO: -4685226.500 (0.122 sec/step)\n"]}]}]}