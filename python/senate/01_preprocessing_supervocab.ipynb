{"cells":[{"cell_type":"code","execution_count":null,"id":"57cfac1c","metadata":{"id":"57cfac1c"},"outputs":[],"source":["import os\n","#import setup_utils as utils\n","import numpy as np\n","import pandas as pd\n","from scipy import sparse\n","from sklearn.feature_extraction.text import CountVectorizer\n","from collections import Counter\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":null,"id":"6d8v1NYTAQeB","metadata":{"id":"6d8v1NYTAQeB"},"outputs":[],"source":["#Unabridged source code originally available at: https://github.com/keyonvafa/tbip\n","# difference to data quoting=3!!!"]},{"cell_type":"code","execution_count":null,"id":"f468fd75","metadata":{"id":"f468fd75"},"outputs":[],"source":["#data source : https://data.stanford.edu/congress_text#download-data\n","#Please download and unzip hein-daily.zip\n","#data diractory where Hein-Daily database is saved\n","data_dir = '/Users/paulhofmarcher/Documents/svn/baR/Projects/Congress_Speeches/hein-daily' \n","#save directory, to be changed as needed\n","save_dir = '/Users/paulhofmarcher/Documents/svn/baR/Projects/Congress_Speeches/data_180322'\n","\n","\n","#predefined set of stopwords\n","stopwords = set(\n","        np.loadtxt(os.path.join(data_dir, \n","                                \"stopwords.txt\"),\n","                   dtype=str,\n","                   delimiter=\"\\n\")) #to be changed approrpriately wherever stopwords are stored\n","\n","#stopwords available at: https://github.com/keyonvafa/tbip/blob/master/setup/stopwords/senate_speeches.txt\n","#to be downloaded and saved to data_dir as defined above\n","\n","#Parameters\n","\n","#minimum number of speeches given by a senator \n","#default value 24\n","min_speeches = 24\n","#minimum number of senators using a bigram\n","#default value 10\n","min_authors_per_word = 10\n","\n","#parameters for count vectorizer\n","min_df = 0.001 #minimum document frequency\n","max_df = 0.3 #maximum document frequency\n","stop_words = stopwords\n","ngram_range = (2, 2) #bigrams only\n","token_pattern = \"[a-zA-Z]+\" \n","\n"]},{"cell_type":"code","execution_count":null,"id":"9a48741c","metadata":{"id":"9a48741c"},"outputs":[],"source":["#Helper function\n","#source code originally available at: https://github.com/keyonvafa/tbip\n","#Count number of occurrences of each value in array of non-negative integers\n","#documentation: https://numpy.org/doc/stable/reference/generated/numpy.bincount.html\n","\n","def bincount_2d(x, weights):\n","    _, num_topics = weights.shape\n","    num_cases = np.max(x) + 1\n","    counts = np.array(\n","      [np.bincount(x, weights=weights[:, topic], minlength=num_cases)\n","       for topic in range(num_topics)])\n","    return counts.T"]},{"cell_type":"code","execution_count":null,"id":"34b7a948","metadata":{"id":"34b7a948"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"6af1592d","metadata":{"id":"6af1592d"},"outputs":[],"source":["\n","    "]},{"cell_type":"code","execution_count":null,"id":"ddf30a5d","metadata":{"id":"ddf30a5d"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"9f42837d","metadata":{"id":"9f42837d","outputId":"7ec9769c-7ec5-4489-ca97-7ec3076d488f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/paulhofmarcher/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n","\n","\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n","b'Skipping line 4272: expected 2 fields, saw 3\\nSkipping line 20062: expected 2 fields, saw 3\\nSkipping line 42459: expected 2 fields, saw 3\\nSkipping line 128479: expected 2 fields, saw 3\\nSkipping line 184232: expected 2 fields, saw 3\\nSkipping line 198989: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 97\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 38606: expected 2 fields, saw 3\\nSkipping line 121720: expected 2 fields, saw 44\\nSkipping line 150102: expected 2 fields, saw 3\\nSkipping line 208534: expected 2 fields, saw 3\\nSkipping line 230417: expected 2 fields, saw 3\\nSkipping line 231036: expected 2 fields, saw 3\\nSkipping line 231523: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 98\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 130853: expected 2 fields, saw 3\\nSkipping line 133567: expected 2 fields, saw 3\\nSkipping line 254360: expected 2 fields, saw 3\\nSkipping line 259692: expected 2 fields, saw 3\\n'\n","b'Skipping line 274300: expected 2 fields, saw 15\\nSkipping line 278643: expected 2 fields, saw 3\\nSkipping line 280818: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 99\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 8628: expected 2 fields, saw 6\\nSkipping line 50417: expected 2 fields, saw 3\\nSkipping line 85318: expected 2 fields, saw 4\\nSkipping line 91950: expected 2 fields, saw 3\\nSkipping line 99609: expected 2 fields, saw 4\\nSkipping line 120098: expected 2 fields, saw 3\\nSkipping line 120710: expected 2 fields, saw 4\\nSkipping line 130940: expected 2 fields, saw 3\\nSkipping line 149048: expected 2 fields, saw 3\\nSkipping line 157111: expected 2 fields, saw 3\\nSkipping line 173213: expected 2 fields, saw 3\\nSkipping line 177119: expected 2 fields, saw 3\\nSkipping line 179075: expected 2 fields, saw 5\\nSkipping line 179463: expected 2 fields, saw 3\\nSkipping line 200426: expected 2 fields, saw 3\\nSkipping line 201310: expected 2 fields, saw 4\\nSkipping line 209273: expected 2 fields, saw 3\\nSkipping line 210370: expected 2 fields, saw 4\\nSkipping line 211440: expected 2 fields, saw 3\\nSkipping line 213738: expected 2 fields, saw 3\\nSkipping line 215061: expected 2 fields, saw 4\\nSkipping line 215886: expected 2 fields, saw 3\\nSkipping line 217711: expected 2 fields, saw 3\\nSkipping line 225964: expected 2 fields, saw 3\\nSkipping line 232179: expected 2 fields, saw 5\\nSkipping line 239119: expected 2 fields, saw 3\\nSkipping line 239441: expected 2 fields, saw 3\\nSkipping line 239695: expected 2 fields, saw 3\\nSkipping line 241731: expected 2 fields, saw 3\\nSkipping line 244544: expected 2 fields, saw 3\\nSkipping line 247328: expected 2 fields, saw 3\\nSkipping line 248952: expected 2 fields, saw 3\\nSkipping line 260882: expected 2 fields, saw 3\\nSkipping line 261460: expected 2 fields, saw 3\\nSkipping line 261464: expected 2 fields, saw 3\\n'\n","b'Skipping line 264637: expected 2 fields, saw 4\\nSkipping line 266010: expected 2 fields, saw 3\\nSkipping line 273692: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 100\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 704: expected 2 fields, saw 3\\nSkipping line 1286: expected 2 fields, saw 3\\nSkipping line 2715: expected 2 fields, saw 3\\nSkipping line 2887: expected 2 fields, saw 3\\nSkipping line 3234: expected 2 fields, saw 3\\nSkipping line 3238: expected 2 fields, saw 3\\nSkipping line 3283: expected 2 fields, saw 3\\nSkipping line 3384: expected 2 fields, saw 3\\nSkipping line 3408: expected 2 fields, saw 4\\nSkipping line 3421: expected 2 fields, saw 4\\nSkipping line 3524: expected 2 fields, saw 3\\nSkipping line 3552: expected 2 fields, saw 3\\nSkipping line 3884: expected 2 fields, saw 3\\nSkipping line 4124: expected 2 fields, saw 3\\nSkipping line 4567: expected 2 fields, saw 3\\nSkipping line 4962: expected 2 fields, saw 3\\nSkipping line 5060: expected 2 fields, saw 3\\nSkipping line 5673: expected 2 fields, saw 3\\nSkipping line 7621: expected 2 fields, saw 3\\nSkipping line 9163: expected 2 fields, saw 3\\nSkipping line 11912: expected 2 fields, saw 3\\nSkipping line 14248: expected 2 fields, saw 3\\nSkipping line 19734: expected 2 fields, saw 3\\nSkipping line 21092: expected 2 fields, saw 3\\nSkipping line 26912: expected 2 fields, saw 3\\nSkipping line 28489: expected 2 fields, saw 3\\nSkipping line 50868: expected 2 fields, saw 3\\nSkipping line 67104: expected 2 fields, saw 4\\nSkipping line 67105: expected 2 fields, saw 3\\nSkipping line 69929: expected 2 fields, saw 3\\nSkipping line 103970: expected 2 fields, saw 3\\nSkipping line 105567: expected 2 fields, saw 3\\nSkipping line 110683: expected 2 fields, saw 3\\nSkipping line 118151: expected 2 fields, saw 3\\nSkipping line 118645: expected 2 fields, saw 3\\nSkipping line 122526: expected 2 fields, saw 3\\nSkipping line 156290: expected 2 fields, saw 3\\nSkipping line 175427: expected 2 fields, saw 3\\nSkipping line 241675: expected 2 fields, saw 11\\nSkipping line 251212: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 101\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 221: expected 2 fields, saw 4\\nSkipping line 8905: expected 2 fields, saw 3\\nSkipping line 12004: expected 2 fields, saw 3\\nSkipping line 14029: expected 2 fields, saw 4\\nSkipping line 40831: expected 2 fields, saw 3\\nSkipping line 40890: expected 2 fields, saw 3\\nSkipping line 65448: expected 2 fields, saw 3\\nSkipping line 71982: expected 2 fields, saw 3\\nSkipping line 76038: expected 2 fields, saw 3\\nSkipping line 76645: expected 2 fields, saw 4\\nSkipping line 80990: expected 2 fields, saw 3\\nSkipping line 84991: expected 2 fields, saw 3\\nSkipping line 87692: expected 2 fields, saw 3\\nSkipping line 109950: expected 2 fields, saw 3\\nSkipping line 117402: expected 2 fields, saw 3\\nSkipping line 135443: expected 2 fields, saw 3\\nSkipping line 148357: expected 2 fields, saw 3\\nSkipping line 149057: expected 2 fields, saw 3\\nSkipping line 166972: expected 2 fields, saw 3\\nSkipping line 190165: expected 2 fields, saw 3\\nSkipping line 212084: expected 2 fields, saw 3\\nSkipping line 212085: expected 2 fields, saw 5\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 102\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 2854: expected 2 fields, saw 3\\nSkipping line 46799: expected 2 fields, saw 3\\nSkipping line 58266: expected 2 fields, saw 3\\nSkipping line 75384: expected 2 fields, saw 3\\nSkipping line 96214: expected 2 fields, saw 3\\nSkipping line 111944: expected 2 fields, saw 3\\nSkipping line 112547: expected 2 fields, saw 4\\nSkipping line 112548: expected 2 fields, saw 3\\nSkipping line 125773: expected 2 fields, saw 3\\nSkipping line 126979: expected 2 fields, saw 3\\nSkipping line 163074: expected 2 fields, saw 3\\nSkipping line 163168: expected 2 fields, saw 4\\nSkipping line 163169: expected 2 fields, saw 3\\nSkipping line 201479: expected 2 fields, saw 3\\nSkipping line 216049: expected 2 fields, saw 4\\nSkipping line 216050: expected 2 fields, saw 3\\nSkipping line 227539: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 103\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 38128: expected 2 fields, saw 3\\nSkipping line 60948: expected 2 fields, saw 4\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 104\n","vocabulary saved for session 105\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 680: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 106\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 46543: expected 2 fields, saw 3\\nSkipping line 47646: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 107\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 315: expected 2 fields, saw 3\\nSkipping line 1177: expected 2 fields, saw 4\\nSkipping line 1180: expected 2 fields, saw 3\\nSkipping line 1602: expected 2 fields, saw 3\\nSkipping line 1706: expected 2 fields, saw 3\\nSkipping line 3800: expected 2 fields, saw 3\\nSkipping line 4085: expected 2 fields, saw 4\\nSkipping line 5539: expected 2 fields, saw 3\\nSkipping line 23353: expected 2 fields, saw 3\\nSkipping line 39940: expected 2 fields, saw 3\\nSkipping line 41117: expected 2 fields, saw 3\\nSkipping line 56606: expected 2 fields, saw 3\\nSkipping line 64448: expected 2 fields, saw 3\\nSkipping line 67966: expected 2 fields, saw 3\\nSkipping line 72444: expected 2 fields, saw 3\\nSkipping line 74706: expected 2 fields, saw 3\\nSkipping line 82804: expected 2 fields, saw 3\\nSkipping line 86682: expected 2 fields, saw 3\\nSkipping line 94020: expected 2 fields, saw 3\\nSkipping line 97727: expected 2 fields, saw 3\\nSkipping line 98896: expected 2 fields, saw 3\\nSkipping line 102205: expected 2 fields, saw 3\\nSkipping line 106483: expected 2 fields, saw 3\\nSkipping line 106712: expected 2 fields, saw 3\\nSkipping line 107598: expected 2 fields, saw 3\\nSkipping line 131997: expected 2 fields, saw 3\\nSkipping line 152495: expected 2 fields, saw 3\\nSkipping line 153950: expected 2 fields, saw 3\\nSkipping line 185665: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 108\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 7151: expected 2 fields, saw 3\\nSkipping line 26149: expected 2 fields, saw 3\\nSkipping line 32866: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 109\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 5787: expected 2 fields, saw 3\\nSkipping line 29405: expected 2 fields, saw 3\\nSkipping line 70043: expected 2 fields, saw 3\\nSkipping line 194087: expected 2 fields, saw 4\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 110\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 42302: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 111\n","vocabulary saved for session 112\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 90057: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 113\n"]},{"name":"stderr","output_type":"stream","text":["b'Skipping line 81801: expected 2 fields, saw 3\\n'\n"]},{"name":"stdout","output_type":"stream","text":["vocabulary saved for session 114\n"]}],"source":["#creating a complete vocabulary covering all the sessions\n","\n","for i in range(97, 115):\n","    if(i < 100):\n","        speeches = pd.read_csv(os.path.join(data_dir, 'speeches_0' + str(i) + '.txt'), \n","                               encoding=\"ISO-8859-1\", \n","                               sep=\"|\",quoting=3,\n","                               on_bad_lines='warn')\n","        description = pd.read_csv(os.path.join(data_dir, 'descr_0' + str(i) + '.txt'), \n","                                  encoding=\"ISO-8859-1\", \n","                                  sep=\"|\")\n","        speaker_map = pd.read_csv(os.path.join(data_dir, '0' + str(i) + '_SpeakerMap.txt'), \n","                                  encoding=\"ISO-8859-1\", \n","                                  sep=\"|\")\n","    else:\n","        speeches = pd.read_csv(os.path.join(data_dir, 'speeches_' + str(i) + '.txt'), \n","                               encoding=\"ISO-8859-1\", \n","                               sep=\"|\", quoting=3,\n","                               on_bad_lines='warn')\n","        description = pd.read_csv(os.path.join(data_dir, 'descr_' + str(i) + '.txt'), \n","                                  encoding=\"ISO-8859-1\", \n","                                  sep=\"|\")\n","        speaker_map = pd.read_csv(os.path.join(data_dir, str(i) + '_SpeakerMap.txt'), \n","                                  encoding=\"ISO-8859-1\", \n","                                  sep=\"|\")\n","\n","    merged_df = speeches.merge(description, \n","                               left_on='speech_id', \n","                               right_on='speech_id')\n","    df = merged_df.merge(speaker_map, left_on='speech_id', right_on='speech_id')\n","    \n","    # Only look at senate speeches.\n","    #to select speakers with speeches in the senate (includes Senators and House Reps)\n","    senate_df = df[df['chamber_x'] == 'S']\n","    #to select ONLY Senators uncomment the next line\n","    #senate_df = df[df['chamber_y'] == 'S'] ##  here 7.2\n","    speaker = np.array(\n","        [' '.join([first, last]) for first, last in \n","         list(zip(np.array(senate_df['firstname']), \n","                  np.array(senate_df['lastname'])))])\n","    speeches = np.array(senate_df['speech'])\n","    party = np.array(senate_df['party'])\n","\n","    # Remove senators who make less than 24 speeches\n","    unique_speaker, speaker_counts = np.unique(speaker, return_counts=True)\n","    absent_speakers = unique_speaker[np.where(speaker_counts < min_speeches)]\n","    absent_speaker_inds = [ind for ind, x in enumerate(speaker) \n","                           if x in absent_speakers]\n","    speaker = np.delete(speaker, absent_speaker_inds)\n","    speeches = np.delete(speeches, absent_speaker_inds)\n","    party = np.delete(party, absent_speaker_inds)\n","    speaker_party = np.array(\n","        [speaker[i] + \" (\" + party[i] + \")\" for i in range(len(speaker))])\n","\n","    # Create mapping between names and IDs.\n","    speaker_to_speaker_id = dict(\n","        [(y.title(), x) for x, y in enumerate(sorted(set(speaker_party)))])\n","    author_indices = np.array(\n","        [speaker_to_speaker_id[s.title()] for s in speaker_party])\n","    author_map = np.array(list(speaker_to_speaker_id.keys()))\n","\n","    count_vectorizer = CountVectorizer(min_df=min_df,\n","                                       max_df=max_df, \n","                                       stop_words=stop_words, \n","                                       ngram_range=ngram_range,\n","                                       token_pattern=token_pattern)\n","    \n","    # Learn initial document term matrix. This is only initial because we use it to\n","    # identify words to exclude based on author counts.\n","    counts = count_vectorizer.fit_transform(speeches.astype(str))\n","    vocabulary = np.array(\n","        [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n","                                key=lambda kv: kv[1])])\n","\n","    # Remove bigrams spoken by less than 10 Senators.\n","    counts_per_author = bincount_2d(author_indices, counts.toarray())\n","    author_counts_per_word = np.sum(counts_per_author > 0, axis=0)\n","    acceptable_words = np.where(\n","        author_counts_per_word >= min_authors_per_word)[0]\n","\n","    # Fit final document-term matrix with modified vocabulary.\n","    count_vectorizer = CountVectorizer(ngram_range=(2, 2),\n","                                       vocabulary=vocabulary[acceptable_words])\n","    counts = count_vectorizer.fit_transform(speeches.astype(str))\n","    vocabulary = np.array(\n","        [k for (k, v) in sorted(count_vectorizer.vocabulary_.items(), \n","                                key=lambda kv: kv[1])])\n","\n","    #counts_dense = remove_cooccurring_ngrams(counts, vocabulary) #not required since only bigrams are being considered \n","    # Remove speeches with no words.\n","    existing_speeches = np.where(np.sum(counts, axis=1) > 0)[0]\n","    counts = counts[existing_speeches]\n","    author_indices = author_indices[existing_speeches]\n","    # session specific vocabulary saved to ~/data\n","    np.savetxt(os.path.join(save_dir, 'vocabulary_' + str(i) + '.txt'), vocabulary, fmt=\"%s\") \n","    print(\"vocabulary saved for session \"+str(i))"]},{"cell_type":"code","execution_count":null,"id":"9362633b","metadata":{"id":"9362633b"},"outputs":[],"source":["#pip install session_info\n","#import session_info\n","#session_info.show()"]},{"cell_type":"code","execution_count":null,"id":"97b88bb5","metadata":{"id":"97b88bb5"},"outputs":[],"source":["#cretae a combined vocabulary for all the sessions\n","\n","super_vocab = [] #empty list\n","\n","for i in range(97, 115):\n","    v = pd.read_csv(os.path.join(save_dir, 'vocabulary_' + str(i) +'.txt'), \n","                    header = None)\n","    v = v[0].tolist()\n","    super_vocab.append(v) #append session specific vocabulary\n","    \n","results_list = super_vocab #list of lists\n","results_union = set().union(*results_list) #set union of lists\n","vocab_full = list(results_union) #change datatype to list\n","vocab_full = sorted(vocab_full) #sorted alphabetically\n","#complete vocabulary saved to ~/data\n","np.savetxt(os.path.join(save_dir, 'vocabulary.txt'), vocab_full, fmt = \"%s\")"]},{"cell_type":"code","execution_count":null,"id":"0a231f4c","metadata":{"id":"0a231f4c"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"01_preprocessing_supervocab.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}